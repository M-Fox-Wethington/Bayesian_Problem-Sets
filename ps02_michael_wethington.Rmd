**Question 1** 

Normal distribution with a known mean $\mu$, but unknown variance $\sigma^2$

Likelihood: $f(x_1,...x_2|\mu,\sigma^2) = f(x_n|\mu,\sigma^2)=\prod \frac{1}{\sqrt{2\pi\sigma^2}}exp\{\frac{-(x_1-\mu)}{2\sigma^2}\}$ (standard for of normal distribution)

$$f(x_n|\mu, \sigma^2)=(2\pi)^-\frac{2}{n}(\sigma^2)e^-\frac{1}{\sigma^2}\sum\frac{(x_i-\mu)^2}{2}$$  liklihood with $\sigma^2$ as parameter of interest

We can implement the inverse gamma prior on $\sigma^2$, with shape parameters $\alpha_0$ and scale parameter $\beta_0$
.$$p(\sigma^2|y,\mu) \propto p(y|\mu, \sigma^2)p(\sigma^2)$$
$$Invgamma(\alpha_1, \beta_1) = Normal(\mu, \sigma^2)\cdot Invgamma(\alpha_0, \beta_0)$$

$$ Inverse\ Gamma: f(Y|\alpha, beta) = \frac{\beta^\alpha}{\gamma(\alpha)}\cdot y^-(\alpha=1)\cdot e^{(-\beta/y)}$$


when we consider the prior, we write it in terms of sigma 2, so instead of y, we will have sigma2 

$$posterior \propto likelihood * prior $$

$$p(\theta|y,\mu)\propto \prod \frac{1}{\sqrt{2\pi\theta}}exp(-\frac{(y_i-\mu)^2}{2\theta})\cdot \frac{\beta_0^{\alpha_0}}{\Gamma()\alpha_0}\theta^{-\alpha_0+1}exp(-\frac{\beta_0}{\theta})$$

$$\propto \prod \frac{1}{\sqrt{2\pi\theta}}exp(-\frac{(y_i-\mu)^2}{2\theta})\cdot \theta^{-\alpha_0+1}exp(-\frac{\beta_0}{\theta})$$

$$= \theta^{-\frac{n}{2}}exp(-\frac{\sum(y_i-\mu)^2}{2\theta})\cdot \theta^{-\alpha_0+1}exp(-\frac{\beta_0}{\theta})$$
$$= \theta^{-(\alpha+\frac{n}{2}+1)}exp(-\frac{\beta_0}{\theta}+\frac{\sum(y_i-\mu)^2}{2\theta})$$

$$= \theta^{-(\alpha+\frac{n}{2}+1)}exp(-\frac{\beta_0+(\frac{\sum(y_i-\mu)^2}{2})}{\theta})$$

$$= \theta^{-(\alpha+\frac{n}{2}+1)}exp(-\frac{2\beta_0+2(\frac{\sum(y_i-\mu)^2}{2})}{2\theta})$$
we have found the inverse gamma distribution!

$$ p(\theta|y,\mu) \propto \theta^{-(\alpha+\frac{n}{2}+1)}exp(-\frac{2\beta_0+2(\frac{\sum(y_i-\mu)^2}{2})}{2\theta})$$

$$\alpha_1 = \alpha_0 + \frac{n}{2}$$
$$\beta_1=\beta_0+\frac{\sum(y_i-\mu)^2}{2}$$
Our posterior is $Invgamma(\alpha_0+\frac{n}{2},\beta_0+\frac{\sum(y_i-\mu)^2}{2} )$



**1b ** 

```{r}
library(MCMCpack)

known.mean <- 68
unknown.sigma.sq <- 16

n.5 <- 5
n.50 <- 50
n.5000 <- 5000

n_list <- list(n.5, n.50, n.5000)

posterior.list <- list()
post.mean.list <- list()
post.var.list <- list()


for(i in n_list){
  norm.samp <- rnorm(i, mean = known.mean, sd=sqrt(unknown.sigma.sq))
  
  #prior alphas
  alpha0 <- 3
  beta0 <- 10
  mu <- 40
  
  #Alpha and Beta parameters after finding the conjugate prior
  alpha1 <- alpha0 + i/2 
  beta1 <- beta0 + sum((norm.samp - known.mean)^2)/2
  
  #Calculate the posterior
  posterior <- rinvgamma(10000, alpha1, beta1)
  posterior.list <- append(posterior.list, posterior)
  post.mean <- mean(posterior)
  post.mean.list <- append(post.mean.list, post.mean)
  post.var <- var(posterior)
  post.var.list <- append(post.var.list, post.var)
  
  print(post.var.list)
  
}


```







**Question 2** 

**2a): Find the MLE of p (r)**

Process:

To find the MLE, maximize $L(\theta|x)$, with respect to $\theta$:

1. calculate the derivative of $L(\theta|x)$ with respect to $\theta$
2. set the derivative equal to zero
3. Solve the resulting equation for $\theta\$

$$ L(x|p) =\prod_{i=1}{}  {n \choose x}{f(x_i|p)={n \choose x}p^x}(1-p)^{1-x}  $$
Becasue the liklihood function relates only to parameter p, 
 $\frac{n!}{x!(n-x)!}$ is considered to be a fixed constant and is removed from consideration.
 
taking the log-liklihood:

$$ LogL(p|x) = l(p|x) = \sum\limits_{i=1}^n  logp+(n-x)\ log(1-p)$$

$$l`(p|x) = \frac{x}{p}+\frac{n-x}{1-p}(-1) $$

we're looking for the maximizer so we need to set the equation to zero and solve for p
$$l`(p|x) = \frac{x}{p}+\frac{n-x}{1-p}(-1) = 0 $$

$$\frac{x}{p}=\frac{n-x}{1-p} \Rightarrow $$

$$ \hat{p}=\frac{x}{n} $$
$$\hat{p}=\frac{3}{12} = .25$$

**2b) Find the MLE using R's 'optim' function**

https://kevintshoemaker.github.io/NRES-746/LAB3.html
https://stackoverflow.com/questions/37552469/r-estimating-parameters-of-binomial-distribution
https://stackoverflow.com/questions/26734744/how-to-plot-the-log-likelihood-of-binomial-distribution

```{r}
library(MASS)

set.seed(100)

#using Bolkers method--- p = 0.25

k = 3
N = 12

binomNLL1 = function(p, k, N){
  -sum(dbinom(k, prob=p, size = N, log=TRUE))
}

bolkers_approach <- optim(fn=binomNLL1, par= c(p=0.5), N=12, k=k) 
bolkers_approach

```

**2c) Brute force (grid search) approach**

```{r}
#Negative likelihood function
binomNLL1 <- function(p) -sum(log(dbinom(3, 12, p)))


#create a sequence to test with
p.seq <- seq(0.01, 0.99, 0.01)

#create a test vector 
Likelihood.storage <- 100

#Loop through sequence to find MLE
for(i in p.seq){
  NLL <- (binomNLL1(i))
  if(NLL < Likelihood.storage){
    Likelihood.storage <- NLL
    p <- i
  }
  
}

print(Likelihood.storage)
print(p)

```


**2D - Bayesian Model Using rJAGS**

```{r}
set.seed(1280)

#############################
#
# Step #1: Set the working directory, and the directory where JAGS lives
#
#############################

jags.directory = "C:/Program Files/JAGS"
setwd("C:/Users/wmichael/Google Drive/BEE569/labs/test")
#############################
#
# Step #2: Load all the libraries and data
#
#############################

library(R2jags)
library(abind)
library(boot)

#
# Step #3: Write the statistical model code to a text file
#
#############################

#sink("PS2_KoalaModel.jags")
# 
# cat("
# model
# {
#   x ~ dbin(p, 12) #data sampled binomially with n=12
#   p ~ dunif(0,1) # prior for the sex ratio of pouch young
# 
# }")

#sink()

#############################
#
# Step #4: Make a list where you include all the data the model will need to run
#
#############################


dat <- list(x = 3)

#############################

#############################
#
# Step #5: Make a function (with no inputs) where you put a list of parameters and their initial values
# these are just starting values for your parameters. Hard coded in this case, where m is drawn from a lognormal distribution
#
#############################

InitStage <- function() {list(p = .5)}
#############################
#
# Step #6: Make a column vector with the names of the parameters you want to track
#
#############################

ParsStage <- c("p")

#############################
#
# Step #7: Set the variables for the MCMC
#
#############################

ni <- 10000  # number of draws from the posterior
nt <- 1    #thinning rate
nb <- 1000  # number to discard for burn-in
nc <- 2  # number of chains

#############################
#
# Step #8: Run the jags function to run the code
#
#############################

model = jags(inits=InitStage,
          n.chains=nc,
          model.file="PS2_KoalaModel.jags",
          working.directory=getwd(),
          data=dat,
          parameters.to.save=ParsStage,
          n.thin=nt,
          n.iter=ni,
          n.burnin=nb,
          DIC=T)
#############################
#
# Step #9: Print the summary and explore the object that was returned
#
#############################

model # prints results of the model

names(model)

names(model$BUGSoutput)

#The mean of the posterior distribution
model.mean <- model$BUGSoutput$mean
model.sd <- model$BUGSoutput$sd

model.mean
model.sd

#Pull out the credible intervals from the summary output
dev.50 <- model$BUGSoutput$summary[row.names(model$BUGSoutput$summary)=='deviance',5] #lower

print("50% Deviance:", dev.50)
print(dev.50)

#Pull out the credible intervals from the summary output
model$BUGSoutput$summary[row.names(model$BUGSoutput$summary)=='p',3] #lower
model$BUGSoutput$summary[row.names(model$BUGSoutput$summary)=='p',7] #upper

```

