---
title: "PS04"
author: "Michael Wethington"
date: "9/16/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r, echo=FALSE}
count <- 0
accept <- c()

sigma = 2 

#target distribution (f(x))
target <- function(x) (1/(2*sigma))*exp(-abs(x)/(sigma))

#proposal distribution (g(x)) --- this envelopes f(x)!
proposal <- function (x) dnorm(x, 0, 6)

#Multiplicative constant with f(x)/g(x) <= M for all x
M <- 4


X <- rnorm(4500, 0, 6)
U <- rnorm (4500, 0, 6)

count <- 1
x.accept <- c()

while(count <= 4500 & length(x.accept) <= 1000){
  test_u <- U[count]
  test_x <- target(X[count])/(4*proposal(X[count]))
  
  if(test_u <= test_x){
    x.accept =  rbind(x.accept, X[count])
    count = count+1
  }
  
count = count+1
}


plot(x.accept, target(x.accept))


```




**Exercise 2**

```{r}
#########################################################

N#QUESTION: SHOULD I USE THE TARGET OR THE PROPOSAL DISTRIBUTION 

#target distribution (f(x))
target <- function(x) (1/(2*sigma))*exp(-abs(x)/(sigma))

#proposal distribution (g(x)) --- this DENSITY envelopes f(x)!
proposal <- function (x) dnorm(x, 0, 6)

sigma = 2

set.seed(1004)

N = 10000

#Step 1. 
# Sample random values from a candidate distribution with support over 
# same range of x values as the target distribution
#########################################################

candidate.values.storage <- c()

for(i in N){
  candidate.val <- rnorm(i, 0, 100) #RANDOM sample
  candidate.values.storage <- c(candidate.values.storage, candidate.val) 
}



#########################################################
#Step 2:
# Find the probability of obtaining those values from a target
# distribution (i.e. the probability density at each x value drawn)
#########################################################
target.pd.storage <- c()

for(i in candidate.values.storage){
  target.pd.sample <- target(i)
  target.pd.storage <- rbind(target.pd.storage, target.pd.sample )
  
}
#########################################################
#Step 3: 
# Normalize the probabilities from Step 2 so they sum to 1
# This is accomplished by dividing each x.i by the sum

#########################################################
normalized.storage <- c()

for(i in target.pd.storage){
  norm.i <- i/sum(target.pd.storage)
  normalized.storage <- c(normalized.storage, norm.i)
}


#########################################################
#Step 4:
# Use (now normalized) probabilities as weights in a resampling of 
# random values from step 1.

# IOW, use sample function with replacement from values drawn
# in step 1, and use the probabilities from step 3 as weights for
# bootstrap sampling
#########################################################

SIR.dist <- c()

#Normalized probabilities 
SIR.dist<- c(sample(candidate.values.storage, 
                    replace = TRUE, 
                    prob = normalized.storage))

#########################################################
#Plot the density
#########################################################
d <- density(SIR.dist)
plot(d)

########################################################
#Exercise 3
#Calculate E[X] from the distribution  
SIR.expected <- sum(sample(length(SIR.dist), SIR.dist, replace = TRUE)/length(SIR.dist))
SIR.mean <- mean(SIR.dist)
SIR.len <- length(SIR.dist)

SIR.Expected <- SIR.mean/SIR.len

########################################################
#Exercise 4
#Calculate the standard error of the SIR
# Boostrap sample many times with replacement from SIR dist
# then find SE of 
########################################################

#bootstrap sample from SIR distribution
SIR.boot <- sample(SIR.dist, 1000, replace = TRUE)

SIR.SE <- ((var(SIR.boot))/(sqrt(length(SIR.dist))))

SIR.SE



```



```{r}
library(gtools)
library(ggplot2)
library(ggExtra)


#purge the env objects
rm(list= ls())

#likelihood function 
L <- function(theta_1, theta_2){
  prod(
    sum(
      (dbinom(2,5,theta_1)*(dbinom(5,5,theta_2))),
      (dbinom(3,5,theta_1)*(dbinom(4,5,theta_2))),
      (dbinom(4,5,theta_1)*(dbinom(3,5,theta_2))),
      (dbinom(5,5,theta_1)*(dbinom(2,5,theta_2)))),
    sum(
      (dbinom(1,6,theta_1)*(dbinom(4,4,theta_2))),
      (dbinom(2,6,theta_1)*(dbinom(3,4,theta_2))),
      (dbinom(3,6,theta_1)*(dbinom(2,4,theta_2))),
      (dbinom(4,6,theta_1)*(dbinom(1,4,theta_2))),
      (dbinom(5,6,theta_1)*(dbinom(0,4,theta_2)))),
    sum(
      (dbinom(0,4,theta_1)*(dbinom(6,6,theta_2))),
      (dbinom(1,4,theta_1)*(dbinom(5,6,theta_2))),
      (dbinom(2,4,theta_1)*(dbinom(4,6,theta_2))),
      (dbinom(3,4,theta_1)*(dbinom(3,6,theta_2))),
      (dbinom(4,4,theta_1)*(dbinom(2,6,theta_2)))))}


###########################################################
#Sample from Priors
###########################################################

N <- 10000

#Storage vectors
Theta1 <- c()
Theta2 <- c()

#Get uniform prior samples for Theta1 and Theta2
for(i in N){
  Theta1 <- c(Theta1, runif(i, 0, 1)) 
  Theta2 <- c(Theta2, runif(i, 0, 1)) 
}

#Generate a proper dataframe
df <- data.frame(Theta1, Theta2)



###########################################################
#Calculate likelihoods using prior values
###########################################################

Likelihoods <- c() 

#Calculate the likelihood of the data for each sample from prior distributions
for(i in 1:length(df$Theta1)){
  L.i <- L(df$Theta1[i],df$Theta2[i])
  Likelihoods <- c(Likelihoods, L.i)
}

#add Likelihoods as column to dataframe
df$Likelihoods <- Likelihoods 


###########################################################
#Normalize the likelihoods 
###########################################################

#find sum of likelihoods so we can normalize them 
Likelihood.sum <- sum(df$Likelihoods)

#Storage vector 
Normalized.L <- c()

#Normalize the Likelihoods 
for(i in 1:length(df$Likelihoods)){
  Normalized.L <- c(Normalized.L, df$Likelihoods[i]/Likelihood.sum)
}

df$Normalized.L <- Normalized.L 


#########################################################################################
#Generate Posteriors:
# Resample from the priors using the normalized probabilities as weights (sample function)
#########################################################################################
Theta1.posterior <- c()
Theta2.posterior <- c()

#Sample from the prior using Normalized probabilities as weights 
Theta1.posterior<- c(sample(Theta1, 
                    replace = TRUE, 
                    prob = Normalized.L))

Theta2.posterior<- c(sample(Theta2, 
                     replace = TRUE, 
                     prob = Normalized.L))

df.posteriors <- data.frame(Theta1.posterior, Theta2.posterior)



###########################################################
#Plot Stuff
###########################################################

#PRIORS
# Prior.plot <- ggplot(data= df, aes(x= df$Theta1, y = df$Theta2))+
#   geom_point()+
#   theme(legend.position = "none")
# 
# #add marginals
# Pr.marginals <- ggMarginal(Prior.plot, type = "histogram")
# Pr.marginals


#POSTERIORS
Posterior.plot <- ggplot(df.posteriors, aes(x= Theta1.posterior, y = Theta2.posterior))+
  geom_point()+
  theme(legend.position = "none")

#add marginals
Posterior.marg <- ggMarginal(Posterior.plot, type = "histogram")
Posterior.marg




#########################
#IS THIS THE POSTERIOR???
#########################
#Recalculate the likelihoods using the new 
# new.Likelihoods <- c() 
# 
# #Calculate the likelihood of the data for each sample from prior distribution
# for(i in 1:length(SIR.dist1)){
#   L.i <- L(SIR.dist1[i],SIR.dist2[i])
#   new.Likelihoods <- c(new.Likelihoods, L.i)
# }
# 
# 
# d <- density(new.Likelihoods)
# 
# plot(d)

```

