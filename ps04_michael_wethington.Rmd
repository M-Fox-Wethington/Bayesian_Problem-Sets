---
title: "PS04"
author: "Michael Wethington"
date: "9/16/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r, echo=FALSE}
count <- 0
accept <- c()

sigma = 2 

#target distribution (f(x))
target <- function(x) (1/(2*sigma))*exp(-abs(x)/(sigma))

#proposal distribution (g(x)) --- this envelopes f(x)!
proposal <- function (x) dnorm(x, 0, 6)

#Multiplicative constant with f(x)/g(x) <= M for all x
M <- 4


X <- rnorm(4500, 0, 6)
U <- rnorm (4500, 0, 6)

count <- 1
x.accept <- c()

while(count <= 4500 & length(x.accept) <= 1000){
  test_u <- U[count]
  test_x <- target(X[count])/(4*proposal(X[count]))
  
  if(test_u <= test_x){
    x.accept =  rbind(x.accept, X[count])
    count = count+1
  }
  
count = count+1
}


plot(x.accept, target(x.accept))


```




**Exercise 2**

```{r}
#########################################################


#target distribution (f(x))
target <- function(x) (1/(2*sigma))*exp(-abs(x)/(sigma))

#proposal distribution (g(x)) --- this DENSITY envelopes f(x)!
proposal <- function (x) dnorm(x, 0, 6)

sigma = 2

set.seed(1004)

N = 10000

#Step 1. 
# Sample random values from a candidate distribution with support over 
# same range of x values as the target distribution
#########################################################

candidate.values.storage <- c()

for(i in N){
  candidate.val <- rnorm(i, 0, 100) #RANDOM sample
  candidate.values.storage <- c(candidate.values.storage, candidate.val) 
}



#########################################################
#Step 2:
# Find the probability of obtaining those values from a target
# distribution (i.e. the probability density at each x value drawn)
#########################################################
target.pd.storage <- c()

for(i in candidate.values.storage){
  target.pd.sample <- target(i)
  target.pd.storage <- rbind(target.pd.storage, target.pd.sample )
  
}
#########################################################
#Step 3: 
# Normalize the probabilities from Step 2 so they sum to 1
# This is accomplished by dividing each x.i by the sum

#########################################################
normalized.storage <- c()

for(i in target.pd.storage){
  norm.i <- i/sum(target.pd.storage)
  normalized.storage <- c(normalized.storage, norm.i)
}


#########################################################
#Step 4:
# Use (now normalized) probabilities as weights in a resampling of 
# random values from step 1.

# IOW, use sample function with replacement from values drawn
# in step 1, and use the probabilities from step 3 as weights for
# bootstrap sampling
#########################################################

SIR.dist <- c()

#Normalized probabilities 
SIR.dist<- c(sample(candidate.values.storage, 
                    replace = TRUE, 
                    prob = normalized.storage))

#########################################################
#Plot the density
#########################################################
d <- density(SIR.dist)
plot(d)

########################################################
#Exercise 3
#Calculate E[X] from the distribution  
SIR.expected <- sum(sample(length(SIR.dist), SIR.dist, replace = TRUE)/length(SIR.dist))
SIR.mean <- mean(SIR.dist)
SIR.len <- length(SIR.dist)

SIR.Expected <- SIR.mean/SIR.len

########################################################
#Exercise 4
#Calculate the standard error of the SIR
# Boostrap sample many times with replacement from SIR dist
# then find SE of 
########################################################



x_samples <- c()
samp <- c()

for(i in 1:1000){
  samp <- sample(x = SIR.dist, size = 1000, replace = TRUE) 
  x_samples[i] <- sum(target(samp))/length(samp)
}
Ex_sd <- sd(x_samples)

Ex_sd

```

**Exercise 5** 

```{r}
library(ggplot2)
library(ggExtra)


#purge the env objects
rm(list= ls())


#Find the MLE by running a grid search

# L <- function(theta_1, theta_2){
#   prod(
#     sum(
#       (dbinom(2,5,theta_1)*(dbinom(5,5,theta_2))),
#       (dbinom(3,5,theta_1)*(dbinom(4,5,theta_2))),
#       (dbinom(4,5,theta_1)*(dbinom(3,5,theta_2))),
#       (dbinom(5,5,theta_1)*(dbinom(2,5,theta_2)))),
#     sum(
#       (dbinom(1,6,theta_1)*(dbinom(4,4,theta_2))),
#       (dbinom(2,6,theta_1)*(dbinom(3,4,theta_2))),
#       (dbinom(3,6,theta_1)*(dbinom(2,4,theta_2))),
#       (dbinom(4,6,theta_1)*(dbinom(1,4,theta_2))),
#       (dbinom(5,6,theta_1)*(dbinom(0,4,theta_2)))),
#     sum(
#       (dbinom(0,4,theta_1)*(dbinom(6,6,theta_2))),
#       (dbinom(1,4,theta_1)*(dbinom(5,6,theta_2))),
#       (dbinom(2,4,theta_1)*(dbinom(4,6,theta_2))),
#       (dbinom(3,4,theta_1)*(dbinom(3,6,theta_2))),
#       (dbinom(4,4,theta_1)*(dbinom(2,6,theta_2)))))}

L <- function(theta_1, theta_2){
  -log(prod(
    sum(
      (dbinom(2,5,theta_1)*(dbinom(5,5,theta_2))),
      (dbinom(3,5,theta_1)*(dbinom(4,5,theta_2))),
      (dbinom(4,5,theta_1)*(dbinom(3,5,theta_2))),
      (dbinom(5,5,theta_1)*(dbinom(2,5,theta_2)))),
    sum(
      (dbinom(1,6,theta_1)*(dbinom(4,4,theta_2))),
      (dbinom(2,6,theta_1)*(dbinom(3,4,theta_2))),
      (dbinom(3,6,theta_1)*(dbinom(2,4,theta_2))),
      (dbinom(4,6,theta_1)*(dbinom(1,4,theta_2))),
      (dbinom(5,6,theta_1)*(dbinom(0,4,theta_2)))),
    sum(
      (dbinom(0,4,theta_1)*(dbinom(6,6,theta_2))),
      (dbinom(1,4,theta_1)*(dbinom(5,6,theta_2))),
      (dbinom(2,4,theta_1)*(dbinom(4,6,theta_2))),
      (dbinom(3,4,theta_1)*(dbinom(3,6,theta_2))),
      (dbinom(4,4,theta_1)*(dbinom(2,6,theta_2))))))}

#Create a matrix to store the values for the different combinations of Theta1 and Theta2
theta1 <- seq(0,1, 0.01) # All possible thetas from 0 to 1 by 0.01 
theta2 <- seq(0,1, 0.01)


likelihood.matrix<-matrix(nrow=length(theta1),ncol=length(theta2))
for(i in 1:length(theta1)){
  for(j in 1:length(theta2)){
    likelihood.matrix[i,j] <- L(theta1[i], theta2[j])
  }
}

likelihood.matrix
MLE <- which(likelihood.matrix==min(likelihood.matrix),arr.ind=T)

MLE

theta1[21] - 0.34
theta2[101] - 0.84


#Create a plot of the likelihood surface
MLE.plot<- image(theta1,theta2,likelihood.matrix,col=topo.colors(100))
contour(theta1,theta2,likelihood.matrix,nlevels=30,add=T)
points(0.34,0.84)

#Plot the likelihood surface
MLE.plot

###########################################################
#Step 1 - Sample from Priors (Uniform(0,1))
###########################################################

N <- 10000
#Storage vectors
Theta1.prior <- c()
Theta2.prior <- c()

#Get uniform prior samples for Theta1 and Theta2
for(i in N){
  Theta1.prior<-runif(N, 0, 1)
  Theta2.prior<-runif(N, 0, 1) 
}

###########################################################
#Step 2 - Generate your likelihood matrix
###########################################################


#Generate a proper dataframe
df <- data.frame(Theta1.prior, Theta2.prior)


###########################################################
#Calculate likelihoods using prior values
###########################################################

Likelihoods <- c() 

#Calculate the likelihood of the data for each sample from prior distributions
for(i in 1:N){
  L.i <- L(df$Theta1.prior[i],df$Theta2.prior[i])
  Likelihoods <- c(Likelihoods, L.i)
}

#add Likelihoods as column to dataframe
df$Likelihoods <- Likelihoods 


###########################################################
#Step 3 - Normalize the likelihoods 
###########################################################

#find sum of likelihoods so we can normalize them 
Likelihood.sum <- sum(df$Likelihoods)

#Storage vector 
Normalized.L <- c()

#Normalize the Likelihoods 
for(i in 1:length(df$Likelihoods)){
  n.ith <- df$Likelihoods[i]/Likelihood.sum
  Normalized.L <- c(Normalized.L, n.ith)
}

#add Normalized likelihoods to the 
df$Normalized.L <- Normalized.L 


#########################################################################################
# Step 4 - #Generate Posteriors:
# # Resample from the priors using the normalized probabilities as weights (sample function)
# #########################################################################################


N <- 10000

resample <- df[sample(1:nrow(df), size = N, replace = T, prob = df$Normalized.L),]

names(resample)[names(resample) == "Theta1.prior"] <- "Posterior1"
names(resample)[names(resample) == "Theta2.prior"] <- "Posterior2"


```
Exercise 6
```{r}
scatterhist = function(x, y, xlab="", ylab="", contour=T, contour.x, contour.y, contour.z){
  zones <- matrix(c(2,0,1,3), ncol=2, byrow=TRUE)
  layout(zones, widths=c(4/5,1/5), heights=c(1/5,4/5))
  xhist <- hist(x, plot=FALSE)
  yhist <- hist(y, plot=FALSE)
  top <- max(c(xhist$counts, yhist$counts))
  par(mar=c(5,5,1,1))
  plot(x,y, pch=16, cex=0.5, col="grey", xlab = xlab, ylab = ylab, cex.lab=1.5)
  if (contour==T) contour(x=contour.x, y=contour.y, z=contour.z, col = "red", nlevels = 30, add=T)
  par(mar=c(0,3,1,1))
  barplot(xhist$counts, axes=FALSE, ylim=c(0, top), space=0)
  par(mar=c(3,0,1,1))
  barplot(yhist$counts, axes=FALSE, xlim=c(0, top), space=0, horiz=TRUE)
  }


#Plot a recreation ofGelman and Smith
scatterhist(x = resample$Posterior1, y = resample$Posterior2,
            contour.x = seq(0,1,0.01),
            contour.y = seq(0,1,0.01),
            contour.z = likelihood.matrix,
            xlab = expression(hat(theta[1])), ylab = expression(hat(theta[2])))
```


**Exercise 7**

Jamming on edges, means the model might be misspecified. The pdf would nt have been equal to 1 since their normalizing of part of the pdf function is off. About support for prior, the prior informs the posterior, but if the value is not present in the prior, it won't be in the posterior.  





**Exercise 8** 
Having aprior of the uniform of point 5 messes because you dont have anything form .5 to 1, it would be like trying to find a max and minumim in a cage, with everything jammed on the edges. Jamming on edges, means the model has been misspecified

